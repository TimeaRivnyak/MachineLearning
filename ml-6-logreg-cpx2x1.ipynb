{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndata=pd.read_csv('../input/heartdisease-2/heart_disease.csv')\nlabels=data.values[:,-1]\nlabels[labels>1]=1\nlabels=labels.astype(int)\n\ndata=data.values[:,:-1]","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-04-01T11:28:18.341433Z","iopub.execute_input":"2022-04-01T11:28:18.341746Z","iopub.status.idle":"2022-04-01T11:28:18.355951Z","shell.execute_reply.started":"2022-04-01T11:28:18.341699Z","shell.execute_reply":"2022-04-01T11:28:18.354861Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"data=PolynomialFeatures(2).fit_transform(data)\nprint(data.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T11:28:18.379372Z","iopub.execute_input":"2022-04-01T11:28:18.379956Z","iopub.status.idle":"2022-04-01T11:28:18.386716Z","shell.execute_reply.started":"2022-04-01T11:28:18.379904Z","shell.execute_reply":"2022-04-01T11:28:18.385621Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"(303, 105)\n","output_type":"stream"}]},{"cell_type":"code","source":"#Standardize data (substract mean divide with std)\ndata=(data-np.mean(data))/np.std(data)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T11:28:18.388546Z","iopub.execute_input":"2022-04-01T11:28:18.389158Z","iopub.status.idle":"2022-04-01T11:28:18.400013Z","shell.execute_reply.started":"2022-04-01T11:28:18.389124Z","shell.execute_reply":"2022-04-01T11:28:18.398638Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"def train_test_split(data,labels,test_ratio=0.2):\n    idxs=np.arange(data.shape[0])\n    np.random.shuffle(idxs)\n    test_idxs=idxs[:round(len(idxs)*test_ratio)]\n    train_idxs=idxs[round(len(idxs)*test_ratio):]\n    return data[train_idxs],labels[train_idxs],data[test_idxs],labels[test_idxs]\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-01T11:28:18.403005Z","iopub.execute_input":"2022-04-01T11:28:18.404025Z","iopub.status.idle":"2022-04-01T11:28:18.410415Z","shell.execute_reply.started":"2022-04-01T11:28:18.403986Z","shell.execute_reply":"2022-04-01T11:28:18.409664Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"def visualize(data, labels,predictor):\n    import matplotlib.pyplot as plt\n    min1, max1 = data[:, 0].min()-data[:, 0].min()*0.1, data[:, 0].max()+data[:, 0].max()*0.1\n    min2, max2 = data[:, 1].min()-data[:, 1].min()*0.1, data[:, 1].max()+data[:, 1].max()*0.1\n    # define the x and y scale\n    x1grid = np.arange(min1, max1, np.abs(max1-min1)*0.001)\n    x2grid = np.arange(min2, max2, np.abs(max2-min2)*0.001)\n    # create all of the lines and rows of the grid\n    xx, yy = np.meshgrid(x1grid, x2grid)\n    # flatten each grid to a vector\n    r1, r2 = xx.flatten(), yy.flatten()\n    r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\n    # horizontal stack vectors to create x1,x2 input for the model\n    grid = np.hstack((r1,r2))\n    # make predictions for the grid\n    yhat = predictor.predict(grid)\n    # reshape the predictions back into a grid\n    zz = yhat.reshape(xx.shape)\n    # plot the grid of x, y and z values as a surface\n    plt.contourf(xx, yy, zz, cmap='Paired')\n    # create scatter plot for samples from each class\n    for class_value in np.unique(labels):\n        # get row indexes for samples with this class\n        row_ix = np.where(labels == class_value)\n        # create scatter of these samples\n        plt.scatter(data[row_ix, 0], data[row_ix, 1], cmap='Paired')\n    plt.tight_layout()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-01T11:28:18.412142Z","iopub.execute_input":"2022-04-01T11:28:18.412527Z","iopub.status.idle":"2022-04-01T11:28:18.428355Z","shell.execute_reply.started":"2022-04-01T11:28:18.412496Z","shell.execute_reply":"2022-04-01T11:28:18.427441Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"class LogisticRegression():\n    def __init__(self):\n        self.w = None\n    def fit(self,data,labels,test_data,test_labels,lambda_val,max_iterations=500):\n        X=data\n        Y=labels\n        step_size=0.05\n        batch_size=32\n        N = X.shape[0]\n        Y = Y.squeeze()\n        assert Y.shape == (N,), (Y.shape, N)\n\n        def gen_batches():\n            inds = np.arange(N)\n            np.random.shuffle(inds)\n            if batch_size is None:\n                yield inds\n            else:\n                for i in range(0, N, batch_size):\n                    yield inds[i:i + batch_size]\n\n        # Initialise w\n        w = np.random.randn(X.shape[1])\n        for it in range(max_iterations):\n            # Train on the permuted dataset\n            avg_error = 0\n            for batch_inds in gen_batches():\n                x = X[batch_inds, ...]\n                y = Y[batch_inds]\n                y_hat = self.predict_proba(x, w)\n                e_in_hat = self.binary_cross_entropy(y,y_hat)\n                gradient_hat = self.error_gradient(x, y,y_hat)\n                avg_error += e_in_hat\n                # Update\n                l2_gradient = 2*lambda_val*w\n                w -= step_size * (gradient_hat + l2_gradient)\n            avg_error /= N\n            Y_hat = (self.predict_proba(X, w) > 0.5).astype(int)\n            accuracy = self.accuracy(Y,Y_hat)\n            test_Y_hat= (self.predict_proba(test_data, w) > 0.5).astype(int)\n            test_accuracy = self.accuracy(test_labels,test_Y_hat)\n#             print(f'Iteration #{it}: average error: {avg_error:0.2f}  '\n#                   f'train accuracy: {accuracy:0.02f}'\n#                  f'test accuracy: {test_accuracy:0.02f}')\n        self.w=w\n        return np.linalg.norm(self.w)\n\n\n    def sigmoid(self,data):\n        # return 1 / (1 + np.exp(-h))\n        # Numerically stable version, see:\n        # https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n        h=data\n        mask = h >= 0\n        result = np.zeros_like(h)\n        z = np.exp(-h[mask])\n        result[mask] = 1 / (1+z)\n        z = np.exp(h[~mask])\n        result[~mask] = z / (1+z)\n        return result\n\n    def binary_cross_entropy(self,true,prediction):\n        \"\"\"\n        Works if `Y` is 0 or 1.\n        \"\"\"\n        return np.sum(-np.log(true*prediction + (1-true) * (1-prediction) + 1e-6))\n\n    def error_gradient(self,data,true,prediction):\n        N = data.shape[0]\n        assert prediction.shape == (N,), prediction.shape\n        assert true.shape == (N,), true.shape\n        assert np.all((true == 0) | (true == 1))\n        t = prediction - true\n        t.shape = (N, 1)\n        elementwise_gradient = t * data\n        return np.mean(elementwise_gradient, axis=0)\n\n    def predict_proba(self,data,w=[]):\n        if len(w)>0:pass\n        else: w=self.w\n        return self.sigmoid(data @ w)\n    def predict(self,data):\n        return np.rint(self.sigmoid(data @ self.w))\n    def accuracy(self,true,prediction):\n        return np.mean(true == prediction)\n ","metadata":{"execution":{"iopub.status.busy":"2022-04-01T11:28:18.845545Z","iopub.execute_input":"2022-04-01T11:28:18.845997Z","iopub.status.idle":"2022-04-01T11:28:18.868731Z","shell.execute_reply.started":"2022-04-01T11:28:18.845964Z","shell.execute_reply":"2022-04-01T11:28:18.867526Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"train_data,train_labels,test_data,test_labels = train_test_split(data,labels)\nlambda_values = [0.00001,0.0001,0.001,0.01,0.1,0.5,1]\nfor lambda_val in lambda_values:\n    print(f\"Lambda: {lambda_val}\")\n    lr=LogisticRegression()\n    w_mag = lr.fit(train_data,train_labels,test_data,test_labels,lambda_val)\n    prediction_in=lr.predict(train_data)\n    prediction_out=lr.predict(test_data)\n    print(lr.accuracy(train_labels,prediction_in))\n    print(lr.accuracy(test_labels,prediction_out))\n    print(f\"Weight magnitude: {w_mag}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-04-01T11:28:18.870762Z","iopub.execute_input":"2022-04-01T11:28:18.871205Z","iopub.status.idle":"2022-04-01T11:28:26.085159Z","shell.execute_reply.started":"2022-04-01T11:28:18.871160Z","shell.execute_reply":"2022-04-01T11:28:26.084205Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"Lambda: 1e-05\n0.7148760330578512\n0.6885245901639344\nWeight magnitude: 10.663906799597138\nLambda: 0.0001\n0.6859504132231405\n0.6557377049180327\nWeight magnitude: 10.356070865099998\nLambda: 0.001\n0.731404958677686\n0.6885245901639344\nWeight magnitude: 7.495482199384309\nLambda: 0.01\n0.6115702479338843\n0.4918032786885246\nWeight magnitude: 1.3696646454141506\nLambda: 0.1\n0.6983471074380165\n0.6721311475409836\nWeight magnitude: 0.4932565617570975\nLambda: 0.5\n0.5785123966942148\n0.47540983606557374\nWeight magnitude: 0.17401132670355196\nLambda: 1\n0.5785123966942148\n0.4918032786885246\nWeight magnitude: 0.09951186725201466\n","output_type":"stream"}]},{"cell_type":"markdown","source":"With increasing lambda the weight magnitude gets smaller.","metadata":{}}]}