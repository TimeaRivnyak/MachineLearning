{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndata=pd.read_csv('../input/heartdisease-2/heart_disease.csv')\nlabels=data.values[:,-1]\nlabels[labels>1]=1\nlabels=labels.astype(int)\n\ndata=data.values[:,:-1]","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-04-06T08:36:38.444916Z","iopub.execute_input":"2022-04-06T08:36:38.445215Z","iopub.status.idle":"2022-04-06T08:36:38.456702Z","shell.execute_reply.started":"2022-04-06T08:36:38.445185Z","shell.execute_reply":"2022-04-06T08:36:38.455962Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"data=PolynomialFeatures(2).fit_transform(data)\nprint(data.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:36:38.512599Z","iopub.execute_input":"2022-04-06T08:36:38.513451Z","iopub.status.idle":"2022-04-06T08:36:38.519466Z","shell.execute_reply.started":"2022-04-06T08:36:38.513366Z","shell.execute_reply":"2022-04-06T08:36:38.518548Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stdout","text":"(303, 105)\n","output_type":"stream"}]},{"cell_type":"code","source":"#Standardize data (substract mean divide with std)\ndata=(data-np.mean(data))/np.std(data)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:36:38.568320Z","iopub.execute_input":"2022-04-06T08:36:38.568997Z","iopub.status.idle":"2022-04-06T08:36:38.574627Z","shell.execute_reply.started":"2022-04-06T08:36:38.568943Z","shell.execute_reply":"2022-04-06T08:36:38.574048Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"def train_test_split(data,labels,test_ratio=0.2):\n    idxs=np.arange(data.shape[0])\n    np.random.shuffle(idxs)\n    test_idxs=idxs[:round(len(idxs)*test_ratio)]\n    train_idxs=idxs[round(len(idxs)*test_ratio):]\n    return data[train_idxs],labels[train_idxs],data[test_idxs],labels[test_idxs]\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:36:38.612014Z","iopub.execute_input":"2022-04-06T08:36:38.612696Z","iopub.status.idle":"2022-04-06T08:36:38.620467Z","shell.execute_reply.started":"2022-04-06T08:36:38.612644Z","shell.execute_reply":"2022-04-06T08:36:38.619405Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"def visualize(data, labels,predictor):\n    import matplotlib.pyplot as plt\n    min1, max1 = data[:, 0].min()-data[:, 0].min()*0.1, data[:, 0].max()+data[:, 0].max()*0.1\n    min2, max2 = data[:, 1].min()-data[:, 1].min()*0.1, data[:, 1].max()+data[:, 1].max()*0.1\n    # define the x and y scale\n    x1grid = np.arange(min1, max1, np.abs(max1-min1)*0.001)\n    x2grid = np.arange(min2, max2, np.abs(max2-min2)*0.001)\n    # create all of the lines and rows of the grid\n    xx, yy = np.meshgrid(x1grid, x2grid)\n    # flatten each grid to a vector\n    r1, r2 = xx.flatten(), yy.flatten()\n    r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\n    # horizontal stack vectors to create x1,x2 input for the model\n    grid = np.hstack((r1,r2))\n    # make predictions for the grid\n    yhat = predictor.predict(grid)\n    # reshape the predictions back into a grid\n    zz = yhat.reshape(xx.shape)\n    # plot the grid of x, y and z values as a surface\n    plt.contourf(xx, yy, zz, cmap='Paired')\n    # create scatter plot for samples from each class\n    for class_value in np.unique(labels):\n        # get row indexes for samples with this class\n        row_ix = np.where(labels == class_value)\n        # create scatter of these samples\n        plt.scatter(data[row_ix, 0], data[row_ix, 1], cmap='Paired')\n    plt.tight_layout()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:36:38.667303Z","iopub.execute_input":"2022-04-06T08:36:38.667933Z","iopub.status.idle":"2022-04-06T08:36:38.680433Z","shell.execute_reply.started":"2022-04-06T08:36:38.667882Z","shell.execute_reply":"2022-04-06T08:36:38.679569Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"class LogisticRegression():\n    def __init__(self):\n        self.w = None\n    def fit(self,data,labels,test_data,test_labels,lambda_val,reg_term=None,max_iterations=500):\n        X=data\n        Y=labels\n        step_size=0.05\n        batch_size=32\n        N = X.shape[0]\n        Y = Y.squeeze()\n        assert Y.shape == (N,), (Y.shape, N)\n\n        def gen_batches():\n            inds = np.arange(N)\n            np.random.shuffle(inds)\n            if batch_size is None:\n                yield inds\n            else:\n                for i in range(0, N, batch_size):\n                    yield inds[i:i + batch_size]\n\n        # Initialise w\n        w = np.random.randn(X.shape[1])\n        for it in range(max_iterations):\n            # Train on the permuted dataset\n            avg_error = 0\n            for batch_inds in gen_batches():\n                x = X[batch_inds, ...]\n                y = Y[batch_inds]\n                y_hat = self.predict_proba(x, w)\n                e_in_hat = self.binary_cross_entropy(y,y_hat)\n                gradient_hat = self.error_gradient(x, y,y_hat)\n                avg_error += e_in_hat\n                # Update\n                if reg_term == 'l1':\n                    for i in range(w.shape[0]-1):\n                        if w[i] > 0:\n                            w[i] -= step_size * (gradient_hat[i] + lambda_val)\n                        elif w[i] < 0:\n                            w[i] -= step_size * (gradient_hat[i] - lambda_val)\n                        elif w[i] == 0:\n                            w[i] -= step_size * gradient_hat[i]\n                elif reg_term == 'l2':\n                    l2_gradient = 2*lambda_val*w\n                    w -= step_size * (gradient_hat + l2_gradient)\n                elif reg_term == None:\n                    w -= step_size * gradient_hat\n            avg_error /= N\n            Y_hat = (self.predict_proba(X, w) > 0.5).astype(int)\n            accuracy = self.accuracy(Y,Y_hat)\n            test_Y_hat= (self.predict_proba(test_data, w) > 0.5).astype(int)\n            test_accuracy = self.accuracy(test_labels,test_Y_hat)\n#             print(f'Iteration #{it}: average error: {avg_error:0.2f}  '\n#                   f'train accuracy: {accuracy:0.02f}  '\n#                   f'test accuracy: {test_accuracy:0.02f}')\n        self.w=w\n        return np.linalg.norm(self.w)\n\n\n    def sigmoid(self,data):\n        # return 1 / (1 + np.exp(-h))\n        # Numerically stable version, see:\n        # https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n        h=data\n        mask = h >= 0\n        result = np.zeros_like(h)\n        z = np.exp(-h[mask])\n        result[mask] = 1 / (1+z)\n        z = np.exp(h[~mask])\n        result[~mask] = z / (1+z)\n        return result\n\n    def binary_cross_entropy(self,true,prediction):\n        \"\"\"\n        Works if `Y` is 0 or 1.\n        \"\"\"\n        return np.sum(-np.log(true*prediction + (1-true) * (1-prediction) + 1e-6))\n\n    def error_gradient(self,data,true,prediction):\n        N = data.shape[0]\n        assert prediction.shape == (N,), prediction.shape\n        assert true.shape == (N,), true.shape\n        assert np.all((true == 0) | (true == 1))\n        t = prediction - true\n        t.shape = (N, 1)\n        elementwise_gradient = t * data\n        return np.mean(elementwise_gradient, axis=0)\n\n    def predict_proba(self,data,w=[]):\n        if len(w)>0:pass\n        else: w=self.w\n        return self.sigmoid(data @ w)\n    def predict(self,data):\n        return np.rint(self.sigmoid(data @ self.w))\n    def accuracy(self,true,prediction):\n        return np.mean(true == prediction)\n ","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:36:38.712615Z","iopub.execute_input":"2022-04-06T08:36:38.712931Z","iopub.status.idle":"2022-04-06T08:36:38.740146Z","shell.execute_reply.started":"2022-04-06T08:36:38.712898Z","shell.execute_reply":"2022-04-06T08:36:38.739067Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"train_data,train_labels,test_data,test_labels = train_test_split(data,labels)\nlambda_values = [0.00001,0.0001,0.001,0.01,0.1,0.5,1]\nreg_term = 'l1'\nprint(reg_term)\nfor lambda_val in lambda_values:\n    print(f\"Lambda: {lambda_val}\")\n    lr=LogisticRegression()\n    w_mag = lr.fit(train_data,train_labels,test_data,test_labels,lambda_val,reg_term)\n    prediction_in=lr.predict(train_data)\n    prediction_out=lr.predict(test_data)\n    print(lr.accuracy(train_labels,prediction_in))\n    print(lr.accuracy(test_labels,prediction_out))\n    print(f\"Weight magnitude: {w_mag}\")\n\nreg_term = 'l2'\nprint(reg_term)\nfor lambda_val in lambda_values:\n    print(f\"Lambda: {lambda_val}\")\n    lr=LogisticRegression()\n    w_mag = lr.fit(train_data,train_labels,test_data,test_labels,lambda_val,reg_term)\n    prediction_in=lr.predict(train_data)\n    prediction_out=lr.predict(test_data)\n    print(lr.accuracy(train_labels,prediction_in))\n    print(lr.accuracy(test_labels,prediction_out))\n    print(f\"Weight magnitude: {w_mag}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:36:38.755700Z","iopub.execute_input":"2022-04-06T08:36:38.756204Z","iopub.status.idle":"2022-04-06T08:37:05.065872Z","shell.execute_reply.started":"2022-04-06T08:36:38.756158Z","shell.execute_reply":"2022-04-06T08:37:05.064977Z"},"trusted":true},"execution_count":119,"outputs":[{"name":"stdout","text":"l1\nLambda: 1e-05\n0.7066115702479339\n0.6885245901639344\nWeight magnitude: 10.746270175322044\nLambda: 0.0001\n0.6818181818181818\n0.6229508196721312\nWeight magnitude: 10.104231548918118\nLambda: 0.001\n0.6694214876033058\n0.639344262295082\nWeight magnitude: 8.993993751128276\nLambda: 0.01\n0.6859504132231405\n0.7213114754098361\nWeight magnitude: 1.7262739164544776\nLambda: 0.1\n0.6115702479338843\n0.5573770491803278\nWeight magnitude: 0.8522389513263396\nLambda: 0.5\n0.5495867768595041\n0.5081967213114754\nWeight magnitude: 1.4385983593017986\nLambda: 1\n0.5495867768595041\n0.5081967213114754\nWeight magnitude: 0.48432419075163535\nl2\nLambda: 1e-05\n0.7066115702479339\n0.639344262295082\nWeight magnitude: 8.776531925120887\nLambda: 0.0001\n0.71900826446281\n0.7377049180327869\nWeight magnitude: 10.173935175858285\nLambda: 0.001\n0.7107438016528925\n0.639344262295082\nWeight magnitude: 8.20904291760787\nLambda: 0.01\n0.7066115702479339\n0.7540983606557377\nWeight magnitude: 1.3009396071395714\nLambda: 0.1\n0.6776859504132231\n0.6557377049180327\nWeight magnitude: 0.5193961811971869\nLambda: 0.5\n0.6694214876033058\n0.639344262295082\nWeight magnitude: 0.1830714487080221\nLambda: 1\n0.4628099173553719\n0.4918032786885246\nWeight magnitude: 0.1150269754922554\n","output_type":"stream"}]},{"cell_type":"markdown","source":"* With increasing lambda the weight magnitude gets smaller.\n1. **HW**:\n* l1 is slower than l2\n* l2 yielded to lower overall weight magnitude than l1.\n* With some lambda value l2 resulted in better accuracies, while l1 was more consistent.","metadata":{}}]}